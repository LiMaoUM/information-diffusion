{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(file=\"../../echo-chamber/data/mastodon.trump.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from fast_langdetect import detect_language\n",
    "md = []\n",
    "for post in data:\n",
    "    if not post[\"in_reply_to_id\"]:\n",
    "        reply = False\n",
    "    else:\n",
    "        reply = True\n",
    "    content = post[\"content\"]\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    # Remove all 'a' tags entirely\n",
    "    for a_tag in soup.find_all(\"a\"):\n",
    "        a_tag.unwrap()\n",
    "\n",
    "    # Get the plain text\n",
    "    plain_text = soup.get_text(separator=\" \")\n",
    "\n",
    "    # Regular expression to match and remove URLs\n",
    "    cleaned_content = re.sub(\n",
    "        r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F])|\\s)+\",\n",
    "        \"\",\n",
    "        plain_text,\n",
    "    )\n",
    "\n",
    "    # Remove excess spaces left by removing the link\n",
    "    cleaned_content = re.sub(r\"\\s+\", \" \", cleaned_content).strip()\n",
    "    lang = detect_language(cleaned_content)\n",
    "    if cleaned_content:\n",
    "        md.append([post[\"id\"], cleaned_content, lang, reply, post.get(\"in_reply_to_id\", None),])\n",
    "\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "md_df = pd.DataFrame(md, columns=[\"id\", \"post\", \"lang\", \"reply\", \"in_reply_to_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BlueSky\n",
    "with open(file=\"../../echo-chamber/data/bsky.trump.json\") as f:\n",
    "    bsky = json.load(f)\n",
    "\n",
    "bsky_data = []\n",
    "from fast_langdetect import detect_language\n",
    "\n",
    "for post in bsky:\n",
    "    if not post[\"record\"].get(\"reply\", None):\n",
    "        reply = False\n",
    "    else:\n",
    "        reply = True\n",
    "    if post[\"record\"].get(\"text\", None):\n",
    "        language = post[\"record\"].get(\"langs\", None)\n",
    "        language = language[0] if language else None\n",
    "        if not language:\n",
    "            language = detect_language(\n",
    "                post[\"record\"][\"text\"].replace(\"\\n\", \" \")\n",
    "            ).lower()\n",
    "        bsky_data.append([post[\"_id\"], post[\"record\"][\"text\"], language, reply, post[\"record\"].get(\"reply\", {}).get(\"parent\", {}).get('uri',None),])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "bsky_df = pd.DataFrame(bsky_data, columns=[\"id\", \"post\", \"lang\", \"reply\", \"in_reply_to_id\"])\n",
    "\n",
    "\n",
    "\n",
    "#Truth Social\n",
    "with open(file=\"../../echo-chamber/data/truthsocial.trump.json\") as f:\n",
    "    ts = json.load(f)\n",
    "\n",
    "ts_data = []\n",
    "for post in ts:\n",
    "    if post[\"in_reply_to_id\"] or post[\"quote_id\"]:\n",
    "        reply = True\n",
    "    else:\n",
    "        reply = False\n",
    "    content = post[\"content\"]\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    # Remove all 'a' tags entirely\n",
    "    for a_tag in soup.find_all(\"a\"):\n",
    "        a_tag.unwrap()\n",
    "\n",
    "    # Get the plain text\n",
    "    plain_text = soup.get_text(separator=\" \")\n",
    "\n",
    "    # Regular expression to match and remove URLs\n",
    "    cleaned_content = re.sub(\n",
    "        r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F])|\\s)+\",\n",
    "        \"\",\n",
    "        plain_text,\n",
    "    )\n",
    "\n",
    "    # Remove excess spaces left by removing the link\n",
    "    cleaned_content = re.sub(r\"\\s+\", \" \", cleaned_content).strip()\n",
    "    lang = detect_language(cleaned_content.replace(\"\\n\", \" \").lower()).lower()\n",
    "    if cleaned_content:\n",
    "        ts_data.append([post[\"id\"], cleaned_content, lang, reply, post.get(\"in_reply_to_id\", None),])\n",
    "\n",
    "ts_df = pd.DataFrame(ts_data, columns=[\"id\", \"post\", 'lang', 'reply', 'in_reply_to_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the english posts\n",
    "bsky_posts = list(bsky_df.loc[(bsky_df[\"lang\"] == \"en\"), \"post\"].dropna())\n",
    "\n",
    "# drop post contain \"event\":\"initializing\",\"ts\n",
    "bsky_posts = [post for post in bsky_posts if 'event\":\"initializing\",\"ts\"' not in post]\n",
    "bsky_posts = [\n",
    "    post\n",
    "    for post in bsky_posts\n",
    "    if detect_language(post.replace(\"\\n\", \"\")).lower() == \"en\"\n",
    "]\n",
    "# ts_df posts remove digits and hyperlinks\n",
    "ts_posts = list(ts_df[\"post\"].dropna())\n",
    "\n",
    "\n",
    "all_posts = bsky_posts + ts_posts\n",
    "target_posts = [\n",
    "    post for post in all_posts if \"trump\" in post.lower() or \"biden\" in post.lower()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import cudf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from cuml.manifold import UMAP\n",
    "from cuml.cluster import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.dimensionality import BaseDimensionalityReduction\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster_model = KMeans(n_clusters=20)\n",
    "# Fit BERTopic without actually performing any dimensionality reduction\n",
    "empty_dimensionality_model = BaseDimensionalityReduction()\n",
    "\n",
    "# # Initialize multilingual stopwords\n",
    "# languages = ['EN', 'JA', 'DE', 'FR', 'PT', 'IT', 'ZH']\n",
    "\n",
    "# # Define custom stopword lists for each language\n",
    "# stopword_lists = {\n",
    "#     'EN': stopwords.words('english'),\n",
    "#     'DE': stopwords.words('german'),\n",
    "#     'FR': stopwords.words('french'),\n",
    "#     'PT': stopwords.words('portuguese'),\n",
    "#     'IT': stopwords.words('italian'),\n",
    "#     # For unsupported languages, provide your own stopword lists\n",
    "#     'JA': ['これ', 'それ', 'あれ', 'この', 'その', 'あの', 'ここ', 'そこ', 'あそこ', 'こちら', 'どれ', 'なぜ', 'なに', 'どうして'],  # Example\n",
    "#     'ZH': ['的', '一', '是', '在', '了', '和', '有', '不', '人', '我', '他', '这', '个', '上', '们', '来'],  # Example\n",
    "# }\n",
    "# # Combine and deduplicate stopwords\n",
    "# all_stopwords = set()\n",
    "# for lang, stopword_list in stopword_lists.items():\n",
    "#     all_stopwords.update(stopword_list)\n",
    "\n",
    "# all_stopwords = list(all_stopwords)  # Convert back to a list if needed\n",
    "all_stopwords = stopwords.words(\"english\")\n",
    "custom_stopwords = [\n",
    "    \"com\",\n",
    "    \"www\",\n",
    "    \"2024\",\n",
    "    \"http\",\n",
    "    \"https\",\n",
    "    \"bsky\",\n",
    "    \"social\",\n",
    "]  # Add domain-specific stopwords\n",
    "all_stopwords.extend(custom_stopwords)\n",
    "all_stopwords = list(set(all_stopwords))  # Deduplicate again\n",
    "\n",
    "\n",
    "# # Initialize the CountVectorizer with the custom stopwords\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=all_stopwords, min_df=2, ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "umap_model = UMAP(n_components=15, n_neighbors=5, min_dist=0.1)\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=200,\n",
    "    min_samples=10,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True,\n",
    ")\n",
    "# Combine posts from each component\n",
    "# bsky_df posts remove digits and hyperlinks and only keep en\n",
    "bsky_posts = list(bsky_df.loc[(bsky_df[\"lang\"] == \"en\"), \"post\"].dropna())\n",
    "\n",
    "# drop post contain \"event\":\"initializing\",\"ts\n",
    "bsky_posts = [post for post in bsky_posts if 'event\":\"initializing\",\"ts\"' not in post]\n",
    "bsky_posts = [\n",
    "    post\n",
    "    for post in bsky_posts\n",
    "    if detect_language(post.replace(\"\\n\", \"\")).lower() == \"en\"\n",
    "]\n",
    "# ts_df posts remove digits and hyperlinks\n",
    "ts_posts = list(ts_df[\"post\"].dropna())\n",
    "\n",
    "\n",
    "all_posts = bsky_posts + ts_posts\n",
    "target_posts = [\n",
    "    post for post in all_posts if \"trump\" in post.lower() or \"biden\" in post.lower()\n",
    "]\n",
    "# Fit BERTopic model\n",
    "embeddings = embedding_model.encode(target_posts, show_progress_bar=True)\n",
    "topic_model = BERTopic(\n",
    "    umap_model=empty_dimensionality_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    embedding_model=embedding_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    verbose=True,\n",
    ")\n",
    "topics, probs = topic_model.fit_transform(target_posts, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topics = topic_model.reduce_outliers(target_posts, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.update_topics(\n",
    "    target_posts, topics=new_topics, vectorizer_model=vectorizer_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we randomly sampled 10 posts from teach cateory to make sense of the topic label\n",
    "# label topics\n",
    "# Topic -1: Criticism of Trump and Support for Democratic Policies\n",
    "# Topic 0: MAGA and Pro-Trump Hashtags and Advocacy\n",
    "# Topic 1: Trump’s Legal Convictions and Felony Charges\n",
    "# Topic 2: Pro-Trump and MAGA Advocacy\n",
    "# Topic 3: Celebrations of Trump (e.g., Birthdays and Tributes)\n",
    "# Topic 4: Hunter Biden’s Legal Troubles (e.g., Gun Charges)\n",
    "# Topic 5: U.S. Policy on Ukraine and Russia\n",
    "# Topic 6: Israel-Hamas Conflict and Biden’s Ceasefire Proposal\n",
    "# Topic 7: Trump’s Tax Promises and Election Campaign\n",
    "# Topic 8: Trump’s Rallies and Live Events Coverage\n",
    "# Topic 9: Biden’s Immigration Policies and Executive Orders\n",
    "# Topic 10: Legal Proceedings in Georgia’s 2020 Election Case Against Trump\n",
    "# Topic 11: Biden vs. Trump Presidential Debates\n",
    "topic_id_to_label = {\n",
    "    -1: \"Criticism of Trump and Support for Democratic Policies\",\n",
    "    0: \"MAGA and Pro-Trump Hashtags and Advocacy\",\n",
    "    1: \"Trump’s Legal Convictions and Felony Charges\",\n",
    "    2: \"Pro-Trump and MAGA Advocacy\",\n",
    "    3: \"Celebrations of Trump (e.g., Birthdays and Tributes)\",\n",
    "    4: \"Hunter Biden’s Legal Troubles (e.g., Gun Charges)\",\n",
    "    5: \"U.S. Policy on Ukraine and Russia\",\n",
    "    6: \"Israel-Hamas Conflict and Biden’s Ceasefire Proposal\",\n",
    "    7: \"Trump’s Tax Promises and Election Campaign\",\n",
    "    8: \"Trump’s Rallies and Live Events Coverage\",\n",
    "    9: \"Biden’s Immigration Policies and Executive Orders\",\n",
    "    10: \"Legal Proceedings in Georgia’s 2020 Election Case Against Trump\",\n",
    "    11: \"Biden vs. Trump Presidential Debates\",\n",
    "}\n",
    "topic_model.set_topic_labels(\n",
    "    {\n",
    "        -1: \"Criticism of Trump and Support for Democratic Policies\",\n",
    "        0: \"MAGA and Pro-Trump Hashtags and Advocacy\",\n",
    "        1: \"Trump’s Legal Convictions and Felony Charges\",\n",
    "        2: \"Pro-Trump and MAGA Advocacy\",\n",
    "        3: \"Celebrations of Trump (e.g., Birthdays and Tributes)\",\n",
    "        4: \"Hunter Biden’s Legal Troubles (e.g., Gun Charges)\",\n",
    "        5: \"U.S. Policy on Ukraine and Russia\",\n",
    "        6: \"Israel-Hamas Conflict and Biden’s Ceasefire Proposal\",\n",
    "        7: \"Trump’s Tax Promises and Election Campaign\",\n",
    "        8: \"Trump’s Rallies and Live Events Coverage\",\n",
    "        9: \"Biden’s Immigration Policies and Executive Orders\",\n",
    "        10: \"Legal Proceedings in Georgia’s 2020 Election Case Against Trump\",\n",
    "        11: \"Biden vs. Trump Presidential Debates\",\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
