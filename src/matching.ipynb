{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec40e1a8-60e5-4337-97a3-f53c1481a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(file=\"../../echo-chamber/data/mastodon.trump.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "103609e3-c185-4f62-9e5b-cf40d7586d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "md = []\n",
    "for post in data:\n",
    "    if not post['in_reply_to_id']:\n",
    "        content = post[\"content\"]\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "        # Remove all 'a' tags entirely\n",
    "        for a_tag in soup.find_all(\"a\"):\n",
    "            a_tag.unwrap()\n",
    "    \n",
    "        # Get the plain text\n",
    "        plain_text = soup.get_text(separator=\" \")\n",
    "    \n",
    "        # Regular expression to match and remove URLs\n",
    "        cleaned_content = re.sub(\n",
    "            r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F])|\\s)+\",\n",
    "            \"\",\n",
    "            plain_text,\n",
    "        )\n",
    "    \n",
    "        # Remove excess spaces left by removing the link\n",
    "        cleaned_content = re.sub(r\"\\s+\", \" \", cleaned_content).strip()\n",
    "        md.append([post['id'],cleaned_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a948c6-10ea-4437-a1d2-9b28f8b70bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "md_df = pd.DataFrame(md, columns=['id','post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b813acb2-1b0b-45fc-9115-53e05aeff440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(file=\"../../echo-chamber/data/bsky.trump.json\") as f:\n",
    "    bsky = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5602a542-21f9-46c0-b96b-af44a7454ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsky_data = []\n",
    "from fast_langdetect import detect_language\n",
    "for post in bsky:\n",
    "    if not post['record'].get('reply',None):\n",
    "        if post['record'].get('text',None):\n",
    "            language = post['record'].get('langs',None)\n",
    "            language = language[0] if language else None\n",
    "            if not language:\n",
    "                language = detect_language(post['record']['text'].replace('\\n',' ')).lower()\n",
    "            bsky_data.append([post['_id'],post['record']['text'], language])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf8550b2-65eb-4d67-88d2-547db87f2305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bsky_df = pd.DataFrame(bsky_data, columns=['id','post', 'lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63861a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsky_df['lang'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "975504d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsky_df['post'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90bf1a1d-325d-4d30-b36f-3b893a755401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(file=\"../../echo-chamber/data/truthsocial.trump.json\") as f:\n",
    "    ts = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534ab590-053d-43ae-984a-c5c81473c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "ts_data = []\n",
    "for post in ts:\n",
    "    if post['in_reply_to_id'] or post['quote_id']:\n",
    "        continue\n",
    "    else:\n",
    "        content = post[\"content\"]\n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "        # Remove all 'a' tags entirely\n",
    "        for a_tag in soup.find_all(\"a\"):\n",
    "            a_tag.unwrap()\n",
    "    \n",
    "        # Get the plain text\n",
    "        plain_text = soup.get_text(separator=\" \")\n",
    "    \n",
    "        # Regular expression to match and remove URLs\n",
    "        cleaned_content = re.sub(\n",
    "            r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F])|\\s)+\",\n",
    "            \"\",\n",
    "            plain_text,\n",
    "        )\n",
    "    \n",
    "        # Remove excess spaces left by removing the link\n",
    "        cleaned_content = re.sub(r\"\\s+\", \" \", cleaned_content).strip()\n",
    "        if cleaned_content:\n",
    "            ts_data.append([post['id'],cleaned_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e13c53d4-1ff5-4ec5-b008-972fe0082dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "483be263-f2bd-44e5-ab59-73517772df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = pd.DataFrame(ts_data, columns=['id','post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7fe5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d63009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(bsky_df['post'] == '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf09838e-f7e6-493e-a9f3-4b2def9fb7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Initialize the SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', )  # You can use other SBERT models as well\n",
    "\n",
    "# Step 2: Define your two lists\n",
    "list1 = list(bsky_df['post'])\n",
    "list2 = list(ts_df['post'])\n",
    "\n",
    "# Step 3: Compute embeddings for both lists\n",
    "embeddings1 = model.encode(list1, convert_to_tensor=True, show_progress_bar=True, batch_size=512)\n",
    "embeddings2 = model.encode(list2, convert_to_tensor=True, show_progress_bar=True, batch_size=512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf25342",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Compute cosine similarity\n",
    "cosine_sim = util.cos_sim(embeddings1.cpu(), embeddings2.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c1835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_sim\n",
    "# Set threshold for similarity\n",
    "threshold = 0.75\n",
    "\n",
    "# Filter indices where similarity > threshold\n",
    "row, col = np.where(similarity_matrix > threshold)\n",
    "weights = similarity_matrix[row, col]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf6963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "import networkx as nx\n",
    "# Use sparse representation for performance\n",
    "sparse_sim = coo_matrix((weights, (row, col)), shape=similarity_matrix.shape)\n",
    "\n",
    "# Initialize bipartite graph\n",
    "B = nx.Graph()\n",
    "\n",
    "# Add nodes for Platform A (0 to len(embeddings_a) - 1) and Platform B\n",
    "B.add_nodes_from(range(len(embeddings1)), bipartite=0)\n",
    "B.add_nodes_from(range(len(embeddings1), len(embeddings1) + len(embeddings2)), bipartite=1)\n",
    "\n",
    "# Add edges from sparse similarity data\n",
    "edges = [(r, len(embeddings1) + c, {\"weight\": w}) for r, c, w in zip(sparse_sim.row, sparse_sim.col, sparse_sim.data)]\n",
    "B.add_edges_from(edges)\n",
    "\n",
    "# Print graph information\n",
    "print(f\"Bipartite graph has {B.number_of_nodes()} nodes and {B.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a3142",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(nx.connected_components(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ee5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find component larger than 1\n",
    "components = [c for c in components if len(c) > 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of components larger than 1: {sum(np.array(num_components) > 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be169237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Retrieve posts for each component\n",
    "platform_a_posts = list1\n",
    "platform_b_posts = list2\n",
    "component_posts = []\n",
    "for component in components:\n",
    "    posts = {\n",
    "        \"Platform A\": [platform_a_posts[node] for node in component if node < len(platform_a_posts)],\n",
    "        \"Platform B\": [platform_b_posts[node - len(platform_a_posts)] for node in component if node >= len(platform_a_posts)],\n",
    "    }\n",
    "    component_posts.append(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e358eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "component_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8775614",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9661e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsky_posts = list(bsky_df.loc[(bsky_df['lang'] == 'en'),'post'].dropna())\n",
    "\n",
    "#drop post contain \"event\":\"initializing\",\"ts\n",
    "bsky_posts = [post for post in bsky_posts if \"event\\\":\\\"initializing\\\",\\\"ts\\\"\" not in post]\n",
    "bsky_posts = [post for post in bsky_posts if detect_language(post.replace('\\n','')).lower() == 'en' ]\n",
    "# ts_df posts remove digits and hyperlinks\n",
    "ts_posts = list(ts_df['post'].dropna())\n",
    "\n",
    "\n",
    "all_posts = bsky_posts + ts_posts\n",
    "target_posts = [post for post in all_posts if \"trump\" in post.lower() or \"biden\" in post.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa9a486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import cudf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from cuml.manifold import UMAP\n",
    "from cuml.cluster import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.dimensionality import BaseDimensionalityReduction\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster_model = KMeans(n_clusters=20)\n",
    "# Fit BERTopic without actually performing any dimensionality reduction\n",
    "empty_dimensionality_model = BaseDimensionalityReduction()\n",
    "\n",
    "# # Initialize multilingual stopwords\n",
    "# languages = ['EN', 'JA', 'DE', 'FR', 'PT', 'IT', 'ZH']\n",
    "\n",
    "# # Define custom stopword lists for each language\n",
    "# stopword_lists = {\n",
    "#     'EN': stopwords.words('english'),\n",
    "#     'DE': stopwords.words('german'),\n",
    "#     'FR': stopwords.words('french'),\n",
    "#     'PT': stopwords.words('portuguese'),\n",
    "#     'IT': stopwords.words('italian'),\n",
    "#     # For unsupported languages, provide your own stopword lists\n",
    "#     'JA': ['これ', 'それ', 'あれ', 'この', 'その', 'あの', 'ここ', 'そこ', 'あそこ', 'こちら', 'どれ', 'なぜ', 'なに', 'どうして'],  # Example\n",
    "#     'ZH': ['的', '一', '是', '在', '了', '和', '有', '不', '人', '我', '他', '这', '个', '上', '们', '来'],  # Example\n",
    "# }\n",
    "# # Combine and deduplicate stopwords\n",
    "# all_stopwords = set()\n",
    "# for lang, stopword_list in stopword_lists.items():\n",
    "#     all_stopwords.update(stopword_list)\n",
    "\n",
    "# all_stopwords = list(all_stopwords)  # Convert back to a list if needed\n",
    "all_stopwords = stopwords.words('english')\n",
    "custom_stopwords = ['com', 'www', '2024', 'http', 'https', \"bsky\", \"social\"]  # Add domain-specific stopwords\n",
    "all_stopwords.extend(custom_stopwords)\n",
    "all_stopwords = list(set(all_stopwords))  # Deduplicate again\n",
    "\n",
    "\n",
    "# # Initialize the CountVectorizer with the custom stopwords\n",
    "vectorizer_model = CountVectorizer(stop_words=all_stopwords, min_df=2, ngram_range=(1, 2))\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "umap_model = UMAP(n_components=15, n_neighbors=5, min_dist=0.1)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=200, min_samples=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "# Combine posts from each component\n",
    "# bsky_df posts remove digits and hyperlinks and only keep en\n",
    "bsky_posts = list(bsky_df.loc[(bsky_df['lang'] == 'en'),'post'].dropna())\n",
    "\n",
    "#drop post contain \"event\":\"initializing\",\"ts\n",
    "bsky_posts = [post for post in bsky_posts if \"event\\\":\\\"initializing\\\",\\\"ts\\\"\" not in post]\n",
    "bsky_posts = [post for post in bsky_posts if detect_language(post.replace('\\n','')).lower() == 'en' ]\n",
    "# ts_df posts remove digits and hyperlinks\n",
    "ts_posts = list(ts_df['post'].dropna())\n",
    "\n",
    "\n",
    "all_posts = bsky_posts + ts_posts\n",
    "target_posts = [post for post in all_posts if \"trump\" in post.lower() or \"biden\" in post.lower()]\n",
    "# Fit BERTopic model\n",
    "embeddings = embedding_model.encode(target_posts, show_progress_bar=True)\n",
    "topic_model = BERTopic(\n",
    "    umap_model=empty_dimensionality_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    embedding_model=embedding_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    verbose=True,\n",
    ")\n",
    "topics, probs = topic_model.fit_transform(target_posts, embeddings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ce17ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2431af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topics = topic_model.reduce_outliers(target_posts, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5ee4225",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "298730f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.update_topics(target_posts, topics=new_topics, vectorizer_model=vectorizer_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "976c24d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = topic_model.get_topic_info()\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "795a4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df.to_csv(\"../data/topic_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d334e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label topics\n",
    "# Topic -1: Criticism of Trump and Support for Democratic Policies\n",
    "# Topic 0: MAGA and Pro-Trump Hashtags and Advocacy\n",
    "# Topic 1: Trump’s Legal Convictions and Felony Charges\n",
    "# Topic 2: Pro-Trump and MAGA Advocacy\n",
    "# Topic 3: Celebrations of Trump (e.g., Birthdays and Tributes)\n",
    "# Topic 4: Hunter Biden’s Legal Troubles (e.g., Gun Charges)\n",
    "# Topic 5: U.S. Policy on Ukraine and Russia\n",
    "# Topic 6: Israel-Hamas Conflict and Biden’s Ceasefire Proposal\n",
    "# Topic 7: Trump’s Tax Promises and Election Campaign\n",
    "# Topic 8: Trump’s Rallies and Live Events Coverage\n",
    "# Topic 9: Biden’s Immigration Policies and Executive Orders\n",
    "# Topic 10: Legal Proceedings in Georgia’s 2020 Election Case Against Trump\n",
    "# Topic 11: Biden vs. Trump Presidential Debates\n",
    "topic_id_to_label = {-1: \"Criticism of Trump and Support for Democratic Policies\",\n",
    "                                0: \"MAGA and Pro-Trump Hashtags and Advocacy\",\n",
    "                                1: \"Trump’s Legal Convictions and Felony Charges\",\n",
    "                                2: \"Pro-Trump and MAGA Advocacy\",\n",
    "                                3: \"Celebrations of Trump (e.g., Birthdays and Tributes)\",\n",
    "                                4: \"Hunter Biden’s Legal Troubles (e.g., Gun Charges)\",\n",
    "                                5: \"U.S. Policy on Ukraine and Russia\",\n",
    "                                6: \"Israel-Hamas Conflict and Biden’s Ceasefire Proposal\",\n",
    "                                7: \"Trump’s Tax Promises and Election Campaign\",\n",
    "                                8: \"Trump’s Rallies and Live Events Coverage\",\n",
    "                                9: \"Biden’s Immigration Policies and Executive Orders\",\n",
    "                                10: \"Legal Proceedings in Georgia’s 2020 Election Case Against Trump\",\n",
    "                                11: \"Biden vs. Trump Presidential Debates\"}\n",
    "topic_model.set_topic_labels({-1: \"Criticism of Trump and Support for Democratic Policies\",\n",
    "                                0: \"MAGA and Pro-Trump Hashtags and Advocacy\",\n",
    "                                1: \"Trump’s Legal Convictions and Felony Charges\",\n",
    "                                2: \"Pro-Trump and MAGA Advocacy\",\n",
    "                                3: \"Celebrations of Trump (e.g., Birthdays and Tributes)\",\n",
    "                                4: \"Hunter Biden’s Legal Troubles (e.g., Gun Charges)\",\n",
    "                                5: \"U.S. Policy on Ukraine and Russia\",\n",
    "                                6: \"Israel-Hamas Conflict and Biden’s Ceasefire Proposal\",\n",
    "                                7: \"Trump’s Tax Promises and Election Campaign\",\n",
    "                                8: \"Trump’s Rallies and Live Events Coverage\",\n",
    "                                9: \"Biden’s Immigration Policies and Executive Orders\",\n",
    "                                10: \"Legal Proceedings in Georgia’s 2020 Election Case Against Trump\",\n",
    "                                11: \"Biden vs. Trump Presidential Debates\"})\n",
    "topic_model.get_topic_info().to_csv(\"../data/custom_topic_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baebf934",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_post_topics_df = pd.DataFrame({\"post\": target_posts, \"topic\": new_topics})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea1616af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save models\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "topic_model.save(\"../models/\", serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdfa5f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "topic_model = BERTopic.load(\"../models/\")\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a44609d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efea1800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea29131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic_model.visualize_hierarchy(custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f786b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "# Visualize the documents in 2-dimensional space and show the titles on hover instead of the abstracts\n",
    "# NOTE: You can hide the hover with `hide_document_hover=True` which is especially helpful if you have a large dataset\n",
    "# NOTE: You can also hide the annotations with `hide_annotations=True` which is helpful to see the larger structure\n",
    "topic_model.visualize_documents(titles, reduced_embeddings=reduced_embeddings, custom_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ff05f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_in_null = list(all_post_topics_df.loc[all_post_topics_df['topic'] == -1, 'post'])\n",
    "# run topic model on posts in null\n",
    "topic_model_null = BERTopic(\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    embedding_model=embedding_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    verbose=True,\n",
    ")\n",
    "topics_null, probs_null = topic_model_null.fit_transform(posts_in_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9956ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df_null = topic_model_null.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "feae83c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text, language='english'):\n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs, mentions, hashtags, and special characters\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # URLs\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)  # Mentions and hashtags\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Special characters\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)  # Non-ASCII characters (e.g., emojis)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and short tokens\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    stop_words.update(set(['com', 'www', 'http', 'https', \"bsky\", \"social\", \"bskysocial\",'link','permalink', 'trump','biden']))  # Custom stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Return processed tokens\n",
    "    return tokens\n",
    "\n",
    "documents = target_posts \n",
    "# Preprocess documents\n",
    "preprocessed_docs = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "# Create a dictionary and a corpus\n",
    "dictionary = Dictionary(preprocessed_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
    "#using LDA\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Define a function to compute coherence scores for different topic numbers\n",
    "def compute_coherence_values(corpus, dictionary, texts, start=2, limit=10, step=1):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num_topics,\n",
    "            random_state=42,\n",
    "            passes=20,\n",
    "            alpha='auto',\n",
    "            per_word_topics=True,\n",
    "        )\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(\n",
    "            model=model, texts=texts, dictionary=dictionary, coherence='c_v'\n",
    "        )\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "# Compute coherence scores\n",
    "start, limit, step = 2, 10, 1\n",
    "models, coherence_values = compute_coherence_values(\n",
    "    corpus=corpus, dictionary=dictionary, texts=preprocessed_docs, start=start, limit=limit, step=step\n",
    ")\n",
    "\n",
    "# Print coherence values\n",
    "for num_topics, coherence in zip(range(start, limit, step), coherence_values):\n",
    "    print(f\"Num Topics = {num_topics}, Coherence Score = {coherence}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8e5dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "x = range(start, limit, step)\n",
    "sns.lineplot(x=x, y=coherence_values, marker='o', markerfacecolor='white', markeredgecolor='blue', linestyle='-', )\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.title(\"Coherence Score vs Number of Topics\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c39f08b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the 4 topics result\n",
    "for idx, model in enumerate(models):\n",
    "    if idx == 1:\n",
    "        print(f\"Number of topics: {idx + 2}\")\n",
    "        topics = model.print_topics(num_words=10)\n",
    "        for topic in topics:\n",
    "            print(topic)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b8f88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label the posts with lda topics\n",
    "model = models[1]\n",
    "lda_topics = [model.get_document_topics(corpus[i]) for i in range(len(corpus))]\n",
    "lda_topics = [max(topic, key=lambda x: x[1])[0] for topic in lda_topics]\n",
    "# Create a DataFrame with the topics\n",
    "lda_df = pd.DataFrame({\"post\": documents, \"topic\": lda_topics})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1c8b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign platfomr to the post\n",
    "# using fast way\n",
    "ts_post_set = set(ts_posts)\n",
    "bertopic_df = all_post_topics_df.copy() \n",
    "bertopic_df['platform']  = all_post_topics_df['post'].apply(lambda x: 'ts' if x in ts_post_set else 'bsky')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7953a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df_nonull = ts_df.dropna(subset=['post']).reset_index(drop=True)\n",
    "ts_df_nonull = ts_df_nonull.merge(bertopic_df, on='post', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28f7e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df_nonull.drop_duplicates(subset=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "375fde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df_nonull.dropna(subset=['topic'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1a9636eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df_nonull['topic'] = ts_df_nonull['topic'].astype(int)\n",
    "ts_df_nonull['topic'] = ts_df_nonull['topic'].map(topic_id_to_label)\n",
    "ts_df_nonull.to_csv(\"../data/ts_df_id_topic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1e840f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bsky df\n",
    "bsky_df_nonull = bsky_df.dropna(subset=['post']).reset_index(drop=True)\n",
    "bsky_df_nonull = bsky_df_nonull.merge(bertopic_df, on='post', how='left')\n",
    "bsky_df_nonull.drop_duplicates(subset=['id'], inplace=True)\n",
    "bsky_df_nonull.dropna(subset=['topic'], inplace=True)\n",
    "bsky_df_nonull['topic_label'] = bsky_df_nonull['topic'].apply(lambda x: topic_id_to_label[int(x)])\n",
    "bsky_df_nonull.to_csv(\"../data/bsky_df_id_topic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26729741",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsky_df_nonull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e29b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df_nonull.topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277be9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "# Iterate over components and map posts to their topics and IDs\n",
    "for component in component_topics:\n",
    "    # Process Platform A (bsky)\n",
    "    for post, topic in zip(component[\"Component\"][\"Platform A\"], component[\"Topics\"][:len(component[\"Component\"][\"Platform A\"])]):\n",
    "        match_bsky = bsky_df[bsky_df[\"post\"] == post]\n",
    "        if not match_bsky.empty:\n",
    "            data.append({\"id\": match_bsky[\"id\"].values[0], \"post\": post, \"topic\": topic})\n",
    "    \n",
    "    # Process Platform B (ts)\n",
    "    for post, topic in zip(component[\"Component\"][\"Platform B\"], component[\"Topics\"][len(component[\"Component\"][\"Platform A\"]):]):\n",
    "        match_ts = ts_df[ts_df[\"post\"] == post]\n",
    "        if not match_ts.empty:\n",
    "            data.append({\"id\": match_ts[\"id\"].values[0], \"post\": post, \"topic\": topic})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Collect all posts and their associated topics and component IDs\n",
    "all_data = []\n",
    "for component_id, component in enumerate(component_topics):  # Use enumerate to assign unique component_id\n",
    "    # Add Platform A (bsky) posts\n",
    "    all_data.extend([(component_id, post, topic) for post, topic in zip(\n",
    "        component[\"Component\"][\"Platform A\"], \n",
    "        component[\"Topics\"][:len(component[\"Component\"][\"Platform A\"])]\n",
    "    )])\n",
    "    \n",
    "    # Add Platform B (ts) posts\n",
    "    all_data.extend([(component_id, post, topic) for post, topic in zip(\n",
    "        component[\"Component\"][\"Platform B\"], \n",
    "        component[\"Topics\"][len(component[\"Component\"][\"Platform A\"]):]\n",
    "    )])# Create a DataFrame for all posts with their component_id and topic\n",
    "all_posts_df = pd.DataFrame(all_data, columns=[\"component_id\", \"post\", \"topic\"])\n",
    "\n",
    "# Merge with bsky_df\n",
    "bsky_merged = all_posts_df.merge(bsky_df, on=\"post\", how=\"inner\")\n",
    "\n",
    "# Merge with ts_df\n",
    "ts_merged = all_posts_df.merge(ts_df, on=\"post\", how=\"inner\")\n",
    "\n",
    "# Combine the results\n",
    "export_df = pd.concat([bsky_merged, ts_merged])\n",
    "\n",
    "# Reorder columns and export to CSV\n",
    "export_df = export_df[[\"component_id\", \"id\", \"post\", \"topic\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e8bd38",
   "metadata": {},
   "source": [
    "## Analysis based on control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2678a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bsky_topic_df = pd.read_csv(\"../data/bsky_df_id_topic.csv\")\n",
    "ts_topic_df = pd.read_csv(\"../data/ts_df_id_topic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aff296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bluesky thread api\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "def get_thread(thread_id):\n",
    "    url = f\"https://public.api.bsky.app/xrpc/app.bsky.feed.getPostThread\"\n",
    "    params = {\n",
    "        \"uri\":thread_id,\n",
    "        'depth': 1000\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n",
    "def get_repost(post_id):\n",
    "    url = f\"https://public.api.bsky.app/xrpc/app.bsky.feed.getRepostedBy\"\n",
    "    params = {\n",
    "        \"uri\":post_id,\n",
    "        \"limit\": 100\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_repost(gaza_ids[61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7bae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get respse headers\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b30e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff03e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_thread(export_df['id'][12395])['thread']['replies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e3454",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_repost(\"at://did:plc:m73xlht5hftrkos34nojlchj/app.bsky.feed.post/3lbk3kho7ak2n\").json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d82f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsky_threads = []\n",
    "from tqdm.auto import tqdm\n",
    "for bsky_id in tqdm(bsky_topic_df['id']):\n",
    "    if \"at\" not in bsky_id:\n",
    "        break\n",
    "    thread = get_thread(bsky_id)\n",
    "    bsky_threads.append(thread) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# Function to recursively traverse and add edges to the graph\n",
    "def build_network(graph, thread, parent_id=None):\n",
    "    post = thread['post']\n",
    "    post_id = post['uri']  # Use the URI as a unique identifier\n",
    "    graph.add_node(post_id, text=post['record']['text'], author=post['author']['displayName'])\n",
    "    \n",
    "    if parent_id:  # If there is a parent, add an edge\n",
    "        graph.add_edge(parent_id, post_id)\n",
    "    \n",
    "    # Process replies recursively\n",
    "    for reply in thread.get('replies', []):\n",
    "        build_network(graph, reply, post_id)\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Build the graph\n",
    "for thread in bsky_threads:\n",
    "    try:\n",
    "        build_network(G, thread['thread'])\n",
    "    except:\n",
    "        print(thread)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93006f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [G.subgraph(c).copy() for c in nx.weakly_connected_components(G)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3030ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# Function to calculate width and depth of a tree\n",
    "def calculate_width_and_depth(tree):\n",
    "    # Choose an arbitrary root (assuming it's a directed graph)\n",
    "    roots = [node for node in tree.nodes if tree.in_degree(node) == 0]\n",
    "    if not roots:\n",
    "        return 0, 0  # Empty tree case\n",
    "\n",
    "    root = roots[0]  # Choose the first root\n",
    "    levels = defaultdict(int)\n",
    "    max_depth = 0\n",
    "\n",
    "    # Perform BFS to calculate levels\n",
    "    queue = deque([(root, 0)])  # (node, level)\n",
    "    visited = set()\n",
    "\n",
    "    while queue:\n",
    "        node, depth = queue.popleft()\n",
    "        if node in visited:\n",
    "            continue\n",
    "        visited.add(node)\n",
    "        levels[depth] += 1\n",
    "        max_depth = max(max_depth, depth)\n",
    "        for neighbor in tree.neighbors(node):\n",
    "            queue.append((neighbor, depth + 1))\n",
    "\n",
    "    max_width = max(levels.values())  # Width is the maximum nodes at any level\n",
    "    return max_width, max_depth\n",
    "\n",
    "# Function to analyze all trees in a forest\n",
    "def analyze_forest(graph):\n",
    "    # For directed graphs, use weakly connected components to identify trees\n",
    "    components = [graph.subgraph(c).copy() for c in nx.weakly_connected_components(graph)]\n",
    "    results = []\n",
    "    for i, tree in enumerate(components, 1):\n",
    "        #give me root id\n",
    "        width, depth = calculate_width_and_depth(tree)\n",
    "        results.append({\"Tree_root_id\": \n",
    "                        [node for node in tree.nodes if tree.in_degree(node) == 0],\n",
    "                        \"Tree_id\": i, \"Width\": width, \"Depth\": depth})\n",
    "    return results\n",
    "\n",
    "#resutls = analyze_forest(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd9ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resutls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8374a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bsky_threads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be264bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaza_ids = set(gaza_ids)\n",
    "gaza_bsky = []\n",
    "for post in bsky:\n",
    "    if post['_id'] in gaza_ids:\n",
    "        gaza_bsky.append(post)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec292bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaza_bsky_posts = []\n",
    "for post in gaza_bsky:\n",
    "    gaza_bsky_posts.append([post[\"_id\"], post['record']['text'],post['record']['createdAt'], post['replyCount'], post['repostCount'],])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f7f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaza_bsky_df = pd.DataFrame(gaza_bsky_posts, columns=['id','post','createdAt','replyCount','repostCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d91db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaza_bsky_df['replyCount'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d6ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaza_ts = []\n",
    "from tqdm.auto import tqdm\n",
    "for post in tqdm(ts):\n",
    "    if post[\"_id\"] in ts_topic_df['id']:\n",
    "        gaza_ts.append(post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaza_ts_posts = []\n",
    "for post in gaza_ts:\n",
    "    gaza_ts_posts.append([post[\"_id\"], post['content'],post['created_at'], post['replies_count'], post['reblogs_count'], post['favourites_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd8d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaza_ts_df = pd.DataFrame(gaza_ts_posts, columns=['id','post','createdAt','replyCount','repostCount','favouriteCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de97c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaza_ts_df['replyCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfc8ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df.loc[export_df['component_id'] == 0, 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd24593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the DataFrame\n",
    "export_df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df.to_csv(\"../data/matched_bsky_ts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a9815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save BERTopic model\n",
    "topic_model.save(\"../models/bertopic_model\")\n",
    "\n",
    "# Export topic summary\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info.to_csv(\"../data/bertopic_topic_summary.csv\", index=False)\n",
    "\n",
    "# Extract and save top words for each topic\n",
    "topics_dict = {}\n",
    "for topic_id in topic_info[\"Topic\"]:\n",
    "    if topic_id != -1:  # Skip the \"outlier\" topic (-1)\n",
    "        topics_dict[topic_id] = topic_model.get_topic(topic_id)\n",
    "\n",
    "# Save topic keywords to a CSV\n",
    "topics_keywords = []\n",
    "for topic_id, keywords in topics_dict.items():\n",
    "    keywords_str = \", \".join([f\"{word} ({score:.2f})\" for word, score in keywords])\n",
    "    topics_keywords.append({\"Topic\": topic_id, \"Keywords\": keywords_str})\n",
    "\n",
    "topics_df = pd.DataFrame(topics_keywords)\n",
    "topics_df.to_csv(\"../data/bertopic_keywords.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf40e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b5f0d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5b08b3ec71442786b8133cb5811054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3556587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "def generate_network(nodes):\n",
    "    \"\"\"\n",
    "    Generate a network graph from a list of nodes with 'id', 'in_reply_to_id', and metadata.\n",
    "\n",
    "    :param nodes: List of dictionaries containing:\n",
    "                  - 'id': Unique identifier for the post\n",
    "                  - 'in_reply_to_id': ID of the parent post this replies to\n",
    "                  - Metadata such as username, like counts, etc.\n",
    "    :return: A NetworkX DiGraph with nodes and edges.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()  # Directed graph to represent reply relationships\n",
    "\n",
    "    for node in tqdm(nodes):\n",
    "        # Add the current node with metadata\n",
    "        G.add_node(\n",
    "            int(node['id']),\n",
    "            # username=node['account']['username'],\n",
    "            # display_name=node['account']['display_name'],\n",
    "            # verified=node['account']['verified'],\n",
    "            # followers_count=node['account']['followers_count'],\n",
    "            # statuses_count=node['account']['statuses_count'],\n",
    "            # replies_count=node['replies_count'],\n",
    "            # favourites_count=node['favourites_count'],\n",
    "            # reblogs_count=node['reblogs_count'],\n",
    "            # visibility=node['visibility'],\n",
    "            # content=node['content'],\n",
    "            # created_at=node['created_at'],\n",
    "            # language=node['language'],\n",
    "            # sensitive=node['sensitive'],\n",
    "            # mentions=[mention['username'] for mention in node.get('mentions', [])],\n",
    "        )\n",
    "\n",
    "        # If it replies to another node, add an edge\n",
    "        if node['in_reply_to_id']:\n",
    "            G.add_node(int(node['in_reply_to_id']))  # Add the parent node if it doesn't exist\n",
    "            G.add_edge(int(node['id']), int(node['in_reply_to_id']))\n",
    "\n",
    "    return G\n",
    "\n",
    "# Generate the network graph\n",
    "ts_network = generate_network(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae9f00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a3153418804d89a1de767069588a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "43818"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the root of the trees\n",
    "ts_trees = nx.weakly_connected_components(ts_network)\n",
    "ts_ids = set(ts_topic_df['id'])\n",
    "# filter the tress which the root is in the trump_biden\n",
    "trump_biden_ts_trees = []\n",
    "root_ids = []\n",
    "for tree in tqdm(ts_trees):\n",
    "    #define root node of the tree: no in degree\n",
    "    root = [node for node in tree if ts_network.out_degree(node) == 0]\n",
    "    if (len(root) == 1) and (root[0] in ts_ids):\n",
    "        trump_biden_ts_trees.append(tree)\n",
    "        root_ids.append(root[0])\n",
    "    elif len(root) >= 2:\n",
    "        print(root)\n",
    "\n",
    "len(trump_biden_ts_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2acf3fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using nx combine all the trees\n",
    "ts_combined = nx.compose_all([ts_network.subgraph(tree).copy() for tree in trump_biden_ts_trees])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fe5bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_nodes = list(ts_combined.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c63e3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef9eb9ac1024ad68de7d7b94209c2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3556587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_ts = []\n",
    "ts_nodes = set(ts_nodes)\n",
    "for post in tqdm(ts):\n",
    "    if int(post['_id']) in ts_nodes:\n",
    "        filtered_ts.append(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba6fc39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=\"../data/ts_threads.json\", mode=\"w\") as f:\n",
    "    json.dump(filtered_ts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c0c7a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_results = analyze_forest(ts_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ccf59377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pirnt tree which width larger than 20\n",
    "for tree in ts_results:\n",
    "    if tree['Width'] > 20:\n",
    "        print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0813207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write ts_gaza to json\n",
    "import json\n",
    "with open(\"../data/topic_corpus/bsky_gaza.json\", \"w\") as f:\n",
    "    json.dump(bsky_threads, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e58dec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_forest(bsky_trump_biden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2dfccdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch components 0\n",
    "from tqdm.auto import tqdm\n",
    "trump_biden = export_df.loc[export_df['component_id'] == 0, 'id']\n",
    "\n",
    "bsky_trump_biden = []\n",
    "for bsky_id in tqdm(trump_biden):\n",
    "    if \"at\" not in bsky_id:\n",
    "        break\n",
    "    thread = get_thread(bsky_id)\n",
    "    bsky_trump_biden.append(thread) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c69ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57b337e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/topic_corpus/bsky_trump_biden.json\", \"w\") as f:\n",
    "    json.dump(bsky_trump_biden, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ac505f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# Function to recursively traverse and add edges to the graph\n",
    "def build_network(graph, thread, parent_id=None):\n",
    "    post = thread['post']\n",
    "    post_id = post['uri']  # Use the URI as a unique identifier\n",
    "    graph.add_node(post_id, text=post['record']['text'], author=post['author']['displayName'])\n",
    "    \n",
    "    if parent_id:  # If there is a parent, add an edge\n",
    "        graph.add_edge(parent_id, post_id)\n",
    "    \n",
    "    # Process replies recursively\n",
    "    for reply in thread.get('replies', []):\n",
    "        build_network(graph, reply, post_id)\n",
    "\n",
    "# Create a directed graph\n",
    "bsky_trump_biden_nw = nx.DiGraph()\n",
    "error_count = 0\n",
    "# Build the graph\n",
    "for thread in tqdm(bsky_trump_biden):\n",
    "    try:\n",
    "        build_network(bsky_trump_biden_nw, thread['thread'])\n",
    "    except:\n",
    "        error_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "11d122a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsky_results = analyze_forest(bsky_trump_biden_nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e07eaed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tree in bsky_results:\n",
    "    if tree['Width'] > 20 or tree['Depth'] > 10:\n",
    "        print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b075b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "all_descendants = []\n",
    "# Example usage\n",
    "for root_id in tqdm(trump_biden):\n",
    "    if \"at\" in root_id:\n",
    "        continue\n",
    "    descendants = find_descendants(ts, root_id)\n",
    "    all_descendants.extend(descendants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4c240940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_tree_structure(ts_forest, bsky_forest):\n",
    "    \"\"\"\n",
    "    Compare tree structure metrics (depth, width, size) between two forests.\n",
    "    \n",
    "    :param ts_forest: NetworkX DiGraph for TruthSocial.\n",
    "    :param bsky_forest: NetworkX DiGraph for Bluesky.\n",
    "    \"\"\"\n",
    "    def calculate_metrics(forest):\n",
    "        depths, widths, sizes = [], [], []\n",
    "        for component in nx.weakly_connected_components(forest):\n",
    "            tree = forest.subgraph(component)\n",
    "            root = [node for node in tree.nodes if tree.in_degree(node) == 0][0]\n",
    "            \n",
    "            # Depth\n",
    "            depth = nx.dag_longest_path_length(tree)\n",
    "            depths.append(depth)\n",
    "            \n",
    "            # Width\n",
    "            level_counts = {}\n",
    "            for node in tree.nodes:\n",
    "                level = len(nx.ancestors(tree, node))\n",
    "                level_counts[level] = level_counts.get(level, 0) + 1\n",
    "            widths.append(max(level_counts.values(), default=0))\n",
    "            \n",
    "            # Size\n",
    "            sizes.append(len(tree.nodes))\n",
    "        \n",
    "        return depths, widths, sizes\n",
    "    \n",
    "    ts_depths, ts_widths, ts_sizes = calculate_metrics(ts_forest)\n",
    "    bsky_depths, bsky_widths, bsky_sizes = calculate_metrics(bsky_forest)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.boxplot([ts_depths, bsky_depths], labels=[\"TruthSocial\", \"Bluesky\"])\n",
    "    plt.title(\"Tree Depth Comparison\")\n",
    "    plt.ylabel(\"Depth\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.boxplot([ts_widths, bsky_widths], labels=[\"TruthSocial\", \"Bluesky\"])\n",
    "    plt.title(\"Tree Width Comparison\")\n",
    "    plt.ylabel(\"Width\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.boxplot([ts_sizes, bsky_sizes], labels=[\"TruthSocial\", \"Bluesky\"])\n",
    "    plt.title(\"Tree Size Comparison\")\n",
    "    plt.ylabel(\"Number of Nodes\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6f383adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_tree_structure(ts_network, bsky_trump_biden_nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1f17eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "# Example: Forest as a single directed graph\n",
    "# Edges define relationships between nodes in different trees\n",
    "\n",
    "# Function to compute metrics for a single tree\n",
    "def compute_tree_metrics(tree):\n",
    "    tree_depth = nx.dag_longest_path_length(tree) if nx.is_directed_acyclic_graph(tree) else 0\n",
    "    branching_factor = (\n",
    "        sum(dict(tree.out_degree()).values()) / tree.number_of_nodes()\n",
    "        if tree.number_of_nodes() > 0 else 0\n",
    "    )\n",
    "    root_nodes = [node for node in tree.nodes() if tree.in_degree(node) == 0]\n",
    "    root_activity = len(list(tree.successors(root_nodes[0]))) if root_nodes else 0\n",
    "    leaf_nodes = [node for node in tree.nodes() if tree.out_degree(node) == 0]\n",
    "    num_leaf_nodes = len(leaf_nodes)\n",
    "    \n",
    "    return {\n",
    "        \"tree_depth\": tree_depth,\n",
    "        \"branching_factor\": branching_factor,\n",
    "        \"root_activity\": root_activity,\n",
    "        \"num_leaf_nodes\": num_leaf_nodes,\n",
    "    }\n",
    "\n",
    "# Analyze each tree (connected component) in the forest\n",
    "metrics = []\n",
    "for component in nx.weakly_connected_components(bsky_trump_biden_nw):\n",
    "    subgraph = bsky_trump_biden_nw.subgraph(component)  # Extract subgraph for the tree\n",
    "    metrics.append(compute_tree_metrics(subgraph))\n",
    "\n",
    "# Convert metrics to a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Summarize the metrics across the forest\n",
    "summary = metrics_df.describe()\n",
    "\n",
    "# Display the summary\n",
    "print(\"Summary of Forest Metrics:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ce0f0083",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for component in nx.weakly_connected_components(ts_combined):\n",
    "    subgraph = ts_combined.subgraph(component)  # Extract subgraph for the tree\n",
    "    metrics.append(compute_tree_metrics(subgraph))\n",
    "\n",
    "# Convert metrics to a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Summarize the metrics across the forest\n",
    "summary = metrics_df.describe()\n",
    "\n",
    "# Display the summary\n",
    "print(\"Summary of Forest Metrics:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9c9ea326",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(trump_biden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e1d45317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using igraph to analyze the motifs\n",
    "import igraph as ig\n",
    "# Create an iGraph Graph object from the NetworkX graph\n",
    "ig_graph = ig.Graph(directed=False)\n",
    "ig_graph.add_vertices(list(ts_combined.nodes))\n",
    "ig_graph.add_edges(list(ts_combined.edges))\n",
    "\n",
    "# Find motifs in the graph\n",
    "motifs_ts = ig_graph.motifs_randesu(size=4)\n",
    "\n",
    "# Display the motifs\n",
    "motifs_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "de7ab8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from automotif import AutoMotif\n",
    "from dotmotif import executors\n",
    "import networkx as nx\n",
    "#convert bsky_trump_biden_nw to undirected graph\n",
    "bsky_trump_biden_nw_undirected = nx.Graph(bsky_trump_biden_nw)\n",
    "# Set up AutoMotif for GPU accelerated motif finding\n",
    "motif_finder = AutoMotif(Graph=bsky_trump_biden_nw_undirected, size=4, directed=False, verbose=True, use_GPU=True)\n",
    "\n",
    "# Start finding motifs\n",
    "motifs = motif_finder.find_all_motifs()\n",
    "\n",
    "# # Calculate the Z-Score for the motifs\n",
    "# z_scores = motif_finder.calculate_zscore(num_random_graphs = 30, Executor = executors.NetworkXExecutor)\n",
    "\n",
    "# Display the motifs found\n",
    "motif_finder.display_all_motifs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "86d6d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for bluesky\n",
    "# Create an iGraph Graph object from the NetworkX graph\n",
    "ig_graph = ig.Graph(directed=False)\n",
    "ig_graph.add_vertices(list(bsky_trump_biden_nw.nodes))\n",
    "ig_graph.add_edges(list(bsky_trump_biden_nw.edges))\n",
    "\n",
    "# Find motifs in the graph\n",
    "motifs_bsky = ig_graph.motifs_randesu(size=4)\n",
    "\n",
    "# Display the motifs\n",
    "motifs_bsky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2a08b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from igraph import Graph, plot\n",
    "from pprint import pprint\n",
    "# Number of vertices\n",
    "n = 4\n",
    "\n",
    "# Iterate over all possible isomorphism classes for 4-vertex directed graphs\n",
    "# The number of isomorphism classes for directed graphs with 4 vertices is 218\n",
    "for class_id in range(16):\n",
    "    # Generate the graph for the given isomorphism class\n",
    "    motif_graph = Graph.Isoclass(n=n, cls=class_id, directed=False)\n",
    "    \n",
    "    # Display the graph\n",
    "    pprint(f\"Isomorphism Class ID: {class_id}: {motif_graph}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19042fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "06069698",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig.Graph.Isoclass(n=3, class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8c41accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "# Define the motif graph (e.g., a star)\n",
    "motif = nx.Graph()\n",
    "motif.add_edges_from([(0, 1), (1, 2), (2, 3), (3,4)])\n",
    "motif_star = nx.Graph()\n",
    "motif_star.add_edges_from([(0, 1), (0, 2), (0, 3), (0,4)])\n",
    "#break down to each tree and detect the motif\n",
    "# Iterate over each tree in the forest\n",
    "count = 0\n",
    "count_star = 0\n",
    "for component in nx.connected_components(bsky_trump_biden_nw_undirected):\n",
    "    subgraph = bsky_trump_biden_nw_undirected.subgraph(component)  # Extract subgraph for the tree\n",
    "    # Find all subgraphs isomorphic to the motif\n",
    "    motif_count = 0\n",
    "    matcher = nx.algorithms.isomorphism.GraphMatcher(subgraph, motif)\n",
    "    for match in matcher.subgraph_isomorphisms_iter():\n",
    "        motif_count += 1\n",
    "        count += 1\n",
    "    matcher_star = nx.algorithms.isomorphism.GraphMatcher(subgraph, motif_star)\n",
    "    for match in matcher_star.subgraph_isomorphisms_iter():\n",
    "        count_star += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e5a236a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "265c36c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae3b36",
   "metadata": {},
   "source": [
    "# fetch the reblog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0475d9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "not readable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/bsky_threads.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     bsky_threads \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "\u001b[0;31mUnsupportedOperation\u001b[0m: not readable"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../data/bsky_threads.json\", \"w\") as f:\n",
    "    bsky_threads = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c7a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailab",
   "language": "python",
   "name": "ailab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
